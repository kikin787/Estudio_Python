{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1855d2c6",
   "metadata": {},
   "source": [
    "## Importación de Librerías y Descarga de recursos NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378477b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\velez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\velez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from unicodedata import normalize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6e491",
   "metadata": {},
   "source": [
    "## Funciones de Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc8e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiador(text):\n",
    "    \"\"\"Remueve etiquetas HTML, caracteres no alfanuméricos y conserva emoticones.\"\"\"\n",
    "    text = re.sub('<[^<]*>', '', text)\n",
    "    emoticons = ''.join(re.findall(r'[:;=]-+[\\)\\(pPD]+', text))\n",
    "    text = re.sub(r'\\W+', ' ', text.lower()) + emoticons.replace('-', '')\n",
    "    return text\n",
    "\n",
    "def remover_emoji(text):\n",
    "    \"\"\"Elimina emojis del texto.\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a015eeb4",
   "metadata": {},
   "source": [
    "## Implementación Propia de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae6c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramWord2Vec:\n",
    "    def __init__(self, text, window_size=2, embedding_dim=100, learning_rate=0.01, \n",
    "                 epochs=5, negative_samples=5):\n",
    "        \"\"\"\n",
    "        Implementación del modelo Skip-Gram de Word2Vec con negative sampling\n",
    "        \n",
    "        Args:\n",
    "            text: Lista de oraciones tokenizadas\n",
    "            window_size: Tamaño de la ventana de contexto\n",
    "            embedding_dim: Dimensión de los vectores de palabras\n",
    "            learning_rate: Tasa de aprendizaje para SGD\n",
    "            epochs: Número de épocas de entrenamiento\n",
    "            negative_samples: Número de muestras negativas por muestra positiva\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        # Construir vocabulario\n",
    "        self.build_vocabulary()\n",
    "        \n",
    "        # Inicializar matrices de vectores\n",
    "        self.W1 = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim, \n",
    "                                   (self.vocab_size, self.embedding_dim))\n",
    "        self.W2 = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim,\n",
    "                                   (self.vocab_size, self.embedding_dim))\n",
    "        \n",
    "        # Crear tabla para negative sampling\n",
    "        self.create_negative_sampling_table()\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"Construye el vocabulario a partir del corpus de texto\"\"\"\n",
    "        # Aplanar todas las oraciones en una lista de palabras\n",
    "        words = [word for sentence in self.text for word in sentence]\n",
    "        \n",
    "        # Contar frecuencia de cada palabra\n",
    "        word_counts = Counter(words)\n",
    "        self.total_words = len(words)\n",
    "        \n",
    "        # Crear mapeos palabra → índice y viceversa\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(word_counts.keys())}\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        \n",
    "        # Calcular frecuencias normalizadas para negative sampling\n",
    "        self.word_freqs = np.zeros(self.vocab_size)\n",
    "        for word, count in word_counts.items():\n",
    "            self.word_freqs[self.word_to_idx[word]] = count / self.total_words\n",
    "        \n",
    "        # Elevar a la potencia 3/4 como se menciona en el paper de Word2Vec\n",
    "        self.word_freqs = np.power(self.word_freqs, 0.75)\n",
    "        self.word_freqs = self.word_freqs / np.sum(self.word_freqs)\n",
    "        \n",
    "        print(f\"Tamaño del vocabulario: {self.vocab_size}\")\n",
    "    \n",
    "    def create_negative_sampling_table(self, table_size=1000000):\n",
    "        \"\"\"Crea una tabla para muestreo negativo eficiente\"\"\"\n",
    "        self.neg_table = np.zeros(table_size, dtype=np.int32)\n",
    "        \n",
    "        p = 0\n",
    "        i = 0\n",
    "        \n",
    "        # Llenar la tabla según las frecuencias de palabras\n",
    "        for word_idx in range(self.vocab_size):\n",
    "            p += self.word_freqs[word_idx]\n",
    "            # Calcular cuántas posiciones ocupará esta palabra en la tabla\n",
    "            count = int(p * table_size)\n",
    "            self.neg_table[i:count] = word_idx\n",
    "            i = count\n",
    "            \n",
    "        # Si hay posiciones restantes, llenarlas con la última palabra\n",
    "        if i < table_size:\n",
    "            self.neg_table[i:] = word_idx\n",
    "        \n",
    "        # Mezclar la tabla para mayor aleatoriedad\n",
    "        np.random.shuffle(self.neg_table)\n",
    "    \n",
    "    def generate_training_pairs(self):\n",
    "        \"\"\"Genera pares de entrenamiento (palabra central, palabra de contexto)\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Para cada oración\n",
    "        for sentence in self.text:\n",
    "            word_indices = [self.word_to_idx[word] for word in sentence if word in self.word_to_idx]\n",
    "            \n",
    "            # Para cada palabra en la oración\n",
    "            for center_pos in range(len(word_indices)):\n",
    "                center_word_idx = word_indices[center_pos]\n",
    "                \n",
    "                # Tomar palabras dentro de la ventana\n",
    "                context_start = max(0, center_pos - self.window_size)\n",
    "                context_end = min(len(word_indices), center_pos + self.window_size + 1)\n",
    "                \n",
    "                for context_pos in range(context_start, context_end):\n",
    "                    if context_pos != center_pos:  # No incluir la palabra central como su propio contexto\n",
    "                        context_word_idx = word_indices[context_pos]\n",
    "                        pairs.append((center_word_idx, context_word_idx))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Función sigmoide\"\"\"\n",
    "        clip_x = np.clip(x, -15, 15)  # Prevenir overflow numérico\n",
    "        return 1 / (1 + np.exp(-clip_x))\n",
    "    \n",
    "    def sample_negative_words(self, positive_word_idx, n_samples):\n",
    "        \"\"\"Muestrea palabras negativas (distintas a la palabra positiva)\"\"\"\n",
    "        negative_samples = []\n",
    "        while len(negative_samples) < n_samples:\n",
    "            neg_idx = self.neg_table[np.random.randint(0, len(self.neg_table))]\n",
    "            if neg_idx != positive_word_idx and neg_idx not in negative_samples:\n",
    "                negative_samples.append(neg_idx)\n",
    "        return negative_samples\n",
    "    \n",
    "    def train_pair(self, center_word_idx, context_word_idx):\n",
    "        \"\"\"Entrena el modelo con un par palabra central - palabra contexto\"\"\"\n",
    "        # 1. Obtener vectores actuales\n",
    "        center_vector = self.W1[center_word_idx]\n",
    "        context_vector = self.W2[context_word_idx]\n",
    "        \n",
    "        # 2. Forward pass para el par positivo\n",
    "        dot_product = np.dot(center_vector, context_vector)\n",
    "        sigmoid_val = self.sigmoid(dot_product)\n",
    "        \n",
    "        # 3. Calcular gradiente para el ejemplo positivo (objetivo = 1)\n",
    "        gradient = self.learning_rate * (1 - sigmoid_val)\n",
    "        \n",
    "        # 4. Actualizar vectores del par positivo\n",
    "        center_update = gradient * context_vector\n",
    "        context_update = gradient * center_vector\n",
    "        \n",
    "        self.W1[center_word_idx] += center_update\n",
    "        self.W2[context_word_idx] += context_update\n",
    "        \n",
    "        # 5. Muestrear palabras negativas\n",
    "        negative_indices = self.sample_negative_words(context_word_idx, self.negative_samples)\n",
    "        \n",
    "        # 6. Forward pass y actualización para cada ejemplo negativo\n",
    "        for neg_idx in negative_indices:\n",
    "            neg_vector = self.W2[neg_idx]\n",
    "            \n",
    "            # Forward pass (objetivo = 0)\n",
    "            neg_dot = np.dot(center_vector, neg_vector)\n",
    "            neg_sigmoid = self.sigmoid(neg_dot)\n",
    "            \n",
    "            # Calcular gradiente (objetivo = 0)\n",
    "            neg_gradient = self.learning_rate * (-neg_sigmoid)\n",
    "            \n",
    "            # Actualizar vectores\n",
    "            center_neg_update = neg_gradient * neg_vector\n",
    "            neg_update = neg_gradient * center_vector\n",
    "            \n",
    "            self.W1[center_word_idx] += center_neg_update\n",
    "            self.W2[neg_idx] += neg_update\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Entrena el modelo de Word2Vec\"\"\"\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epochs):\n",
    "            # Generar pares de entrenamiento\n",
    "            training_pairs = self.generate_training_pairs()\n",
    "            \n",
    "            # Mezclar para SGD\n",
    "            random.shuffle(training_pairs)\n",
    "            \n",
    "            print(f\"Época {epoch+1}/{self.epochs}, Pares: {len(training_pairs)}\")\n",
    "            \n",
    "            # Entrenar cada par\n",
    "            for i, (center_idx, context_idx) in enumerate(training_pairs):\n",
    "                self.train_pair(center_idx, context_idx)\n",
    "                \n",
    "                # Mostrar progreso\n",
    "                if (i+1) % 10000 == 0 or i == len(training_pairs) - 1:\n",
    "                    print(f\"  Progreso: {i+1}/{len(training_pairs)}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Entrenamiento completado en {training_time:.2f} segundos\")\n",
    "        self.training_time = training_time\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"Obtiene el vector de una palabra específica\"\"\"\n",
    "        if word in self.word_to_idx:\n",
    "            return self.W1[self.word_to_idx[word]]\n",
    "        else:\n",
    "            return np.zeros(self.embedding_dim)  # Vector de ceros para palabras desconocidas\n",
    "    \n",
    "    def get_document_vector(self, document):\n",
    "        \"\"\"Calcula el vector promedio para un documento (lista de palabras)\"\"\"\n",
    "        vectors = [self.get_word_vector(word) for word in document if word in self.word_to_idx]\n",
    "        if not vectors:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ce006",
   "metadata": {},
   "source": [
    "## Función para evaluación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03aa131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precisión positiva (PPV)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Precisión negativa (NPV)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensibilidad\": sensitivity,\n",
    "        \"Especificidad\": specificity,\n",
    "        \"FPR\": fpr,\n",
    "        \"FNR\": fnr,\n",
    "        \"PPV\": ppv,\n",
    "        \"NPV\": npv\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6c2cb",
   "metadata": {},
   "source": [
    "## Función Para Calcular Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce28cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precisión positiva (PPV)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Precisión negativa (NPV)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensibilidad\": sensitivity,\n",
    "        \"Especificidad\": specificity,\n",
    "        \"FPR\": fpr,\n",
    "        \"FNR\": fnr,\n",
    "        \"PPV\": ppv,\n",
    "        \"NPV\": npv\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c627a",
   "metadata": {},
   "source": [
    "## Función para Comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_word2vec():\n",
    "    # Cargamos el dataset\n",
    "    print(\"Cargando dataset...\")\n",
    "    df = pd.read_csv('movie_data.csv')\n",
    "    print(f\"Dataset cargado. Dimensiones: {df.shape}\")\n",
    "    \n",
    "    # Preprocesamiento\n",
    "    print(\"Aplicando preprocesamiento...\")\n",
    "    df['review'] = df['review'].apply(limpiador)\n",
    "    df['review'] = df['review'].apply(remover_emoji)\n",
    "    df['review'] = df['review'].apply(lambda text: normalize(\"NFKD\", text)\n",
    "                                      .encode(\"ascii\", \"ignore\")\n",
    "                                      .decode(\"utf-8\", \"ignore\"))\n",
    "    \n",
    "    # Tokenización y eliminación de stopwords para ambos modelos\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized = [word_tokenize(corpus) for corpus in df[\"review\"].values]\n",
    "    tokens_sin_stop = [[word for word in sublist if word.lower() not in stop_words] \n",
    "                       for sublist in tokenized]\n",
    "    tokenized_final = [list(filter(lambda x: len(x) > 1, document)) \n",
    "                       for document in tokens_sin_stop]\n",
    "    \n",
    "    # Para trabajar con una muestra más pequeña (opcional)\n",
    "    # sample_size = 1000\n",
    "    # tokenized_final = tokenized_final[:sample_size]\n",
    "    # y = df[\"sentiment\"].values[:sample_size]\n",
    "    \n",
    "    # Etiquetas (sentimientos)\n",
    "    y = df[\"sentiment\"].values\n",
    "    \n",
    "    # Parámetros comunes\n",
    "    feature_size = 100  # Dimensión de los vectores\n",
    "    window_size = 5     # Tamaño de ventana contextual\n",
    "    min_word_count = 5  # Frecuencia mínima de palabras\n",
    "    epochs = 3          # Épocas de entrenamiento (reducido para la implementación propia)\n",
    "    \n",
    "## Modelo 1: Word2Vec de Gensim con Bosques Aleatorios\n",
    "    print(\"\\n### MODELO 1: WORD2VEC (GENSIM) + RANDOM FOREST ###\")\n",
    "    \n",
    "    # Entrenamos el modelo Word2Vec de Gensim\n",
    "    print(\"Entrenando Word2Vec de Gensim...\")\n",
    "    start_time = time.time()\n",
    "    word_vec_gensim = word2vec.Word2Vec(\n",
    "        tokenized_final, \n",
    "        vector_size=feature_size,\n",
    "        window=window_size, \n",
    "        min_count=min_word_count,\n",
    "        epochs=epochs, \n",
    "        seed=42\n",
    "    )\n",
    "    gensim_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Entrenamiento de Word2Vec (Gensim) completado en {gensim_time:.2f} segundos\")\n",
    "    \n",
    "    # Para cada documento, promediamos los vectores de sus palabras\n",
    "    X_gensim = []\n",
    "    for doc in tokenized_final:\n",
    "        valid_words = [word for word in doc if word in word_vec_gensim.wv.key_to_index]\n",
    "        if valid_words:\n",
    "            doc_vector = np.mean([word_vec_gensim.wv[word] for word in valid_words], axis=0)\n",
    "        else:\n",
    "            doc_vector = np.zeros(feature_size)\n",
    "        X_gensim.append(doc_vector)\n",
    "    \n",
    "    X_gensim = np.array(X_gensim)\n",
    "    \n",
    "    # Entrenar Random Forest\n",
    "    print(\"Entrenando Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_gensim, y)\n",
    "    \n",
    "    # Evaluación\n",
    "    y_pred_rf = rf_model.predict(X_gensim)\n",
    "    rf_metrics = calculate_metrics(y, y_pred_rf)\n",
    "    rf_metrics[\"Modelo\"] = \"Word2Vec (Gensim) + Random Forest\"\n",
    "    rf_metrics[\"Tiempo de entrenamiento Word2Vec (s)\"] = gensim_time\n",
    "    \n",
    "    print(f\"Accuracy del modelo Random Forest: {rf_metrics['Accuracy']:.4f}\")\n",
    "\n",
    "## Modelo 2: Word2Vec Propio con MLP\n",
    "    print(\"\\n### MODELO 2: WORD2VEC (PROPIO) + MLP ###\")\n",
    "    \n",
    "    # Entrenar Word2Vec propio\n",
    "    print(\"Entrenando Word2Vec propio...\")\n",
    "    custom_w2v = SkipGramWord2Vec(\n",
    "        text=tokenized_final,\n",
    "        window_size=window_size,\n",
    "        embedding_dim=feature_size,\n",
    "        learning_rate=0.025,\n",
    "        epochs=epochs,\n",
    "        negative_samples=5\n",
    "    )\n",
    "    \n",
    "    custom_w2v.train()\n",
    "    \n",
    "    # Generar vectores de documento promediando vectores de palabras\n",
    "    X_custom = []\n",
    "    for doc in tokenized_final:\n",
    "        doc_vector = custom_w2v.get_document_vector(doc)\n",
    "        X_custom.append(doc_vector)\n",
    "    \n",
    "    X_custom = np.array(X_custom)\n",
    "    \n",
    "    # Entrenar MLP con los vectores generados\n",
    "    print(\"Entrenando MLP...\")\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        batch_size='auto',\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp_model.fit(X_custom, y)\n",
    "    \n",
    "    # Evaluación\n",
    "    y_pred_mlp = mlp_model.predict(X_custom)\n",
    "    mlp_metrics = calculate_metrics(y, y_pred_mlp)\n",
    "    mlp_metrics[\"Modelo\"] = \"Word2Vec (Propio) + MLP\"\n",
    "    mlp_metrics[\"Tiempo de entrenamiento Word2Vec (s)\"] = custom_w2v.training_time\n",
    "    \n",
    "    print(f\"Accuracy del modelo MLP: {mlp_metrics['Accuracy']:.4f}\")\n",
    "\n",
    "## COMPARACIÓN DE RESULTADOS\n",
    "        # Crear DataFrame con resultados\n",
    "    results_df = pd.DataFrame([rf_metrics, mlp_metrics])\n",
    "    \n",
    "    print(\"\\n### COMPARACIÓN DE RESULTADOS ###\")\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    # Visualizar comparación de accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    models = results_df[\"Modelo\"].values\n",
    "    accuracies = results_df[\"Accuracy\"].values\n",
    "    \n",
    "    plt.bar(models, accuracies, color=['skyblue', 'lightgreen'])\n",
    "    plt.title('Comparación de Accuracy entre modelos')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparacion_word2vec.png')\n",
    "    \n",
    "    # Imprimir matriz de confusión para cada modelo\n",
    "    print(\"\\nMatriz de confusión - Random Forest:\")\n",
    "    print(confusion_matrix(y, y_pred_rf))\n",
    "    \n",
    "    print(\"\\nMatriz de confusión - MLP:\")\n",
    "    print(confusion_matrix(y, y_pred_mlp))\n",
    "    \n",
    "    # Reporte de clasificación\n",
    "    print(\"\\nReporte de clasificación - Random Forest:\")\n",
    "    print(classification_report(y, y_pred_rf))\n",
    "    \n",
    "    print(\"\\nReporte de clasificación - MLP:\")\n",
    "    print(classification_report(y, y_pred_mlp))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comparar_word2vec()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
