{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m__init__.pxd:942\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function __import__> returned a result with an exception set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\velez\\OneDrive\\Documentos\\Python\\cargas.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/velez/OneDrive/Documentos/Python/cargas.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/velez/OneDrive/Documentos/Python/cargas.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/velez/OneDrive/Documentos/Python/cargas.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\velez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig_init\u001b[39;00m  \u001b[39m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     \u001b[39m# dtype\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     ArrowDtype,\n\u001b[0;32m     62\u001b[0m     Int8Dtype,\n\u001b[0;32m     63\u001b[0m     Int16Dtype,\n\u001b[0;32m     64\u001b[0m     Int32Dtype,\n\u001b[0;32m     65\u001b[0m     Int64Dtype,\n\u001b[0;32m     66\u001b[0m     UInt8Dtype,\n\u001b[0;32m     67\u001b[0m     UInt16Dtype,\n\u001b[0;32m     68\u001b[0m     UInt32Dtype,\n\u001b[0;32m     69\u001b[0m     UInt64Dtype,\n\u001b[0;32m     70\u001b[0m     Float32Dtype,\n\u001b[0;32m     71\u001b[0m     Float64Dtype,\n\u001b[0;32m     72\u001b[0m     CategoricalDtype,\n\u001b[0;32m     73\u001b[0m     PeriodDtype,\n\u001b[0;32m     74\u001b[0m     IntervalDtype,\n\u001b[0;32m     75\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     76\u001b[0m     StringDtype,\n\u001b[0;32m     77\u001b[0m     BooleanDtype,\n\u001b[0;32m     78\u001b[0m     \u001b[39m# missing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     NA,\n\u001b[0;32m     80\u001b[0m     isna,\n\u001b[0;32m     81\u001b[0m     isnull,\n\u001b[0;32m     82\u001b[0m     notna,\n\u001b[0;32m     83\u001b[0m     notnull,\n\u001b[0;32m     84\u001b[0m     \u001b[39m# indexes\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     Index,\n\u001b[0;32m     86\u001b[0m     CategoricalIndex,\n\u001b[0;32m     87\u001b[0m     RangeIndex,\n\u001b[0;32m     88\u001b[0m     MultiIndex,\n\u001b[0;32m     89\u001b[0m     IntervalIndex,\n\u001b[0;32m     90\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     91\u001b[0m     DatetimeIndex,\n\u001b[0;32m     92\u001b[0m     PeriodIndex,\n\u001b[0;32m     93\u001b[0m     IndexSlice,\n\u001b[0;32m     94\u001b[0m     \u001b[39m# tseries\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     NaT,\n\u001b[0;32m     96\u001b[0m     Period,\n\u001b[0;32m     97\u001b[0m     period_range,\n\u001b[0;32m     98\u001b[0m     Timedelta,\n\u001b[0;32m     99\u001b[0m     timedelta_range,\n\u001b[0;32m    100\u001b[0m     Timestamp,\n\u001b[0;32m    101\u001b[0m     date_range,\n\u001b[0;32m    102\u001b[0m     bdate_range,\n\u001b[0;32m    103\u001b[0m     Interval,\n\u001b[0;32m    104\u001b[0m     interval_range,\n\u001b[0;32m    105\u001b[0m     DateOffset,\n\u001b[0;32m    106\u001b[0m     \u001b[39m# conversion\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     to_numeric,\n\u001b[0;32m    108\u001b[0m     to_datetime,\n\u001b[0;32m    109\u001b[0m     to_timedelta,\n\u001b[0;32m    110\u001b[0m     \u001b[39m# misc\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     Flags,\n\u001b[0;32m    112\u001b[0m     Grouper,\n\u001b[0;32m    113\u001b[0m     factorize,\n\u001b[0;32m    114\u001b[0m     unique,\n\u001b[0;32m    115\u001b[0m     value_counts,\n\u001b[0;32m    116\u001b[0m     NamedAgg,\n\u001b[0;32m    117\u001b[0m     array,\n\u001b[0;32m    118\u001b[0m     Categorical,\n\u001b[0;32m    119\u001b[0m     set_eng_float_format,\n\u001b[0;32m    120\u001b[0m     Series,\n\u001b[0;32m    121\u001b[0m     DataFrame,\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    126\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtseries\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\velez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmissing\u001b[39;00m \u001b[39mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\velez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas_parser\u001b[39;00m  \u001b[39m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas_datetime\u001b[39;00m  \u001b[39m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterval\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmissing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\velez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py:38\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdtypes\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlocalize_pydatetime\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mget_supported_reso\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m ]\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes  \u001b[39m# pylint: disable=import-self\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversion\u001b[39;00m \u001b[39mimport\u001b[39;00m localize_pydatetime\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     Resolution,\n\u001b[0;32m     42\u001b[0m     get_supported_reso,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     periods_per_second,\n\u001b[0;32m     47\u001b[0m )\n",
      "File \u001b[1;32mdtypes.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.dtypes\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mnp_datetime.pyx:29\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.np_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m__init__.pxd:944\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from unicodedata import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos pandas\n",
    "import pandas as pd\n",
    "#Leemos el dataset\n",
    "df = pd.read_csv('Suicide_Detection.csv')\n",
    "#Mostramos las primeras filas\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion auxiliar de limpieza\n",
    "import re\n",
    "def Limpiador(text):\n",
    "    # Remover tags de html\n",
    "    text = re.sub('<[^<]*>','',text)\n",
    "    \n",
    "    # Almacenar temporalmente los emoticons\n",
    "    emoticons = ''.join(re.findall('[:;=]-+[\\)\\(pPD]+',text))\n",
    "    \n",
    "    # Elimine los caracteres que no son palabras y combinar los emoticones\n",
    "    text = re.sub('\\W+',' ',text.lower()) + emoticons.replace('-','')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para remover emojis\n",
    "def RemoverEmoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos los limpiadores\n",
    "df['text'] = df['text'].apply(Limpiador)\n",
    "#Remover Emojis\n",
    "df['text']=df['text'].apply(lambda x: RemoverEmoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar a utf-8, remover acentos\n",
    "from unicodedata import normalize\n",
    "\n",
    "RemoverAcentos = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(RemoverAcentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos datos limpios\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos el toolkit de procesamiento de lenguaje natural\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#Seleccionamos el lenguaje\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el tokenizador de palabras\n",
    "from nltk import word_tokenize\n",
    "#Seleccioanmos idioma\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#Seleccionamos la columna de review\n",
    "corpora = df[\"text\"].values\n",
    "#Tokenizamos todas las entradas del dataset en review\n",
    "tokenized = [word_tokenize(corpus) for corpus in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización y eliminación de stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['text'].apply(lambda x: [word for word in word_tokenize(x.lower()) if word.isalnum() and word not in stop_words])\n",
    "\n",
    "# Eliminamos las palabras de tamaño 1\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if len(word) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los stopwords de todas las entradas del dataset que ya ha sido tokenizado\n",
    "tokens_sin_stopwords = [[word for word in sublist if word.lower() not in stop_words] for sublist in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las palabras de tamaño 1\n",
    "tokenized_final = [list(filter(lambda x: len(x) > 1, document)) \\\n",
    "             for document in tokens_sin_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos un ejemplo tokenizado y sin stopwords\n",
    "print(tokenized_final[2222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cargas(sumaFinal, tamaño_ventana):\n",
    "    numeros_random = []\n",
    "    suma = 0\n",
    "    while suma < sumaFinal:\n",
    "        numeroA = random.randint(1, 30)  # Modificado para que sea de 1 a 30\n",
    "        if suma + numeroA > sumaFinal:\n",
    "            numeros_random.append(sumaFinal - suma)\n",
    "            break\n",
    "        else:\n",
    "            numeros_random.append(numeroA)\n",
    "            suma = sum(numeros_random)\n",
    "    if len(numeros_random) % tamaño_ventana != 0:\n",
    "        resto = len(numeros_random) % tamaño_ventana\n",
    "        if resto != 0:\n",
    "            elementos_faltantes = tamaño_ventana - resto\n",
    "            numeros_random.extend([0] * elementos_faltantes)\n",
    "    return numeros_random\n",
    "\n",
    "def generar_poblacion(num_individuos, tamaño_ventana):\n",
    "    poblacion = []\n",
    "    for _ in range(num_individuos):\n",
    "        individuo = [random.uniform(1, tamaño_ventana) for _ in range(tamaño_ventana)]\n",
    "        poblacion.append(individuo)\n",
    "    return poblacion\n",
    "\n",
    "def calcular_fitness(asignacion_procesos, maxspan):\n",
    "    utilizacion_procesadores = []\n",
    "    for procesos in asignacion_procesos.values():\n",
    "        carga_total = sum(procesos)\n",
    "        utilizacion = carga_total / maxspan\n",
    "        utilizacion_procesadores.append(utilizacion)\n",
    "\n",
    "    apu = sum(utilizacion_procesadores) / len(utilizacion_procesadores)\n",
    "    fitness = (1 / maxspan) * apu\n",
    "    return fitness\n",
    "\n",
    "def seleccion_por_ruleta(poblacion, fitness_poblacion):\n",
    "    total_fitness = sum(fitness_poblacion)\n",
    "    probabilidad_seleccion = [fit / total_fitness for fit in fitness_poblacion]\n",
    "    papa1 = random.choices(poblacion, weights=probabilidad_seleccion)[0]\n",
    "    papa2 = random.choices(poblacion, weights=probabilidad_seleccion)[0]\n",
    "    return papa1, papa2\n",
    "\n",
    "def mutacion(individuo, tasa_mut):\n",
    "    if random.random() < tasa_mut:\n",
    "        idx1, idx2 = random.sample(range(len(individuo)), 2)\n",
    "        individuo[idx1], individuo[idx2] = individuo[idx2], individuo[idx1]\n",
    "    return individuo\n",
    "\n",
    "def cruzamiento(papa1, papa2, tasa_cruz, tasa_mut):\n",
    "    if random.random() < tasa_cruz:\n",
    "        punto_de_cruce = random.randint(1, len(papa1) - 1)\n",
    "\n",
    "        hijo1 = papa1[:punto_de_cruce] + [x for x in papa2 if x not in papa1[:punto_de_cruce]]\n",
    "        hijo2 = papa2[:punto_de_cruce] + [x for x in papa1 if x not in papa2[:punto_de_cruce]]\n",
    "    else:\n",
    "        hijo1 = papa1\n",
    "        hijo2 = papa2\n",
    "\n",
    "    return mutacion(hijo1, tasa_mut), mutacion(hijo2, tasa_mut)\n",
    "\n",
    "def conversion(individuo, carga, tamaño_ventana, num_procesadores):\n",
    "    asignacion_procesos = {i + 1: [] for i in range(num_procesadores)}\n",
    "    indices = list(map(int, individuo))  # No se usa split() porque `individuo` es una lista\n",
    "    x = 0\n",
    "    while x < len(carga):\n",
    "        for i, indice in enumerate(indices):\n",
    "            clave = (i % num_procesadores) + 1\n",
    "            asignacion_procesos[clave].append(carga[(indice - 1) + x])\n",
    "        x = x + tamaño_ventana\n",
    "    maxspan = max(sum(procesos) for procesos in asignacion_procesos.values())\n",
    "    fitness = calcular_fitness(asignacion_procesos, maxspan)\n",
    "    return asignacion_procesos, fitness\n",
    "\n",
    "def slide_window(procesos, size_w):\n",
    "    ventanas = []\n",
    "    for i in range(0, len(procesos), size_w):\n",
    "        ventana = procesos[i:i + size_w]\n",
    "        if len(ventana) < size_w:\n",
    "            ventana += [0] * (size_w - len(ventana))\n",
    "        ventanas.append(ventana)\n",
    "    return ventanas\n",
    "\n",
    "def calcular_estadisticas_procesadores(asignacion_procesos):\n",
    "    colas_procesadores = [procesos for procesos in asignacion_procesos.values()]\n",
    "    media_procesadores = sum(map(sum, colas_procesadores)) / len(colas_procesadores)\n",
    "    max_cola = max(sum(procesos) for procesos in asignacion_procesos.values())\n",
    "    return colas_procesadores, media_procesadores, max_cola\n",
    "\n",
    "def imprimir_colas_procesadores(colas_procesadores):\n",
    "    for i, cola in enumerate(colas_procesadores):\n",
    "        print(f\"Procesador {i} = {sum(cola)}\")\n",
    "\n",
    "def cargar_datos_csv(nombre_archivo):\n",
    "    df = pd.read_csv(nombre_archivo)\n",
    "    num_registros = len(df)\n",
    "    return df, num_registros\n",
    "\n",
    "def distribuir_con_slide_window(num_procesadores):\n",
    "    nombre_archivo_csv = 'Suicide_Detection.csv'  # Reemplaza 'nombre_del_archivo.csv' con el nombre real de tu archivo CSV\n",
    "    df, num_twits = cargar_datos_csv(nombre_archivo_csv)\n",
    "    tamaño_ventana = num_procesadores * 2\n",
    "    num_individuos = 10\n",
    "    num_generaciones = 50\n",
    "    tasa_mut = 0.1\n",
    "    tasa_cruz = 0.8\n",
    "\n",
    "    while num_procesadores <= 8:\n",
    "        carga = cargas(num_twits, tamaño_ventana)\n",
    "\n",
    "        for generacion in range(num_generaciones):\n",
    "            poblacion = generar_poblacion(num_individuos, tamaño_ventana)\n",
    "            mejor_asignacion_procesos = None\n",
    "            mejor_carga_actual = float('-inf')\n",
    "            nueva = None\n",
    "\n",
    "            for individuo in poblacion:\n",
    "                asignacion, _ = conversion(individuo, carga, tamaño_ventana, num_procesadores)\n",
    "                carga_procesadores = [sum(procesos) for procesos in asignacion.values()]\n",
    "                max_carga_procesador = max(carga_procesadores)\n",
    "\n",
    "                if max_carga_procesador > mejor_carga_actual:\n",
    "                    mejor_carga_actual = max_carga_procesador\n",
    "                    mejor_asignacion_procesos = asignacion.copy()\n",
    "                    nueva = individuo\n",
    "\n",
    "            print(f\"\\nGeneración: {generacion + 1}\")\n",
    "            print(\"Detalles de la asignación de procesos:\")\n",
    "            for i, procesos in mejor_asignacion_procesos.items():\n",
    "                print(f\"Procesador {i} = {sum(procesos)}\")\n",
    "            print(f\"Media de las cargas de procesadores: {sum(carga_procesadores) / len(carga_procesadores)}\")\n",
    "            print(f\"Mayor carga encontrada: {mejor_carga_actual}\")\n",
    "            print(f\"Nueva población: {nueva}\")\n",
    "\n",
    "            nueva_poblacion = []\n",
    "\n",
    "            for _ in range(num_individuos):\n",
    "                if random.random() < tasa_cruz:\n",
    "                    papa1, papa2 = seleccion_por_ruleta(poblacion, [1] * num_individuos)\n",
    "                    hijo1, hijo2 = cruzamiento(papa1, papa2, tasa_cruz, tasa_mut)\n",
    "                    nueva_poblacion.extend([hijo1, hijo2])\n",
    "\n",
    "                nueva_poblacion.append(mutacion(nueva, tasa_mut))\n",
    "\n",
    "            poblacion = nueva_poblacion\n",
    "            word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "        num_procesadores += 1\n",
    "\n",
    "    \n",
    "distribuir_con_slide_window(8)  # Cambia el número de procesadores según sea necesario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
